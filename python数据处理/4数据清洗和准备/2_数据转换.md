## 1. 删除重复值

删除重复行

```python
data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                     'k2': [1, 1, 2, 3, 3, 4, 4 ]
                     })
data
Out[5]: 
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
6  two   4
```

DataFrame的duplicated方法返回一个布尔值Series，这个Series反映的是一个行是否存在重复(与之前出现过的行相同)

```python
data.duplicated()
Out[6]: 
0    False
1    False
2    False
3    False
4    False
5    False
6     True
dtype: bool
```

`drop_diplicates`返回的是DataFrame，内容是duplicated返回数组中为False的部分

```python
data.drop_duplicates()
Out[7]: 
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4

```

这个方法默认都是对列进行操作的，你可以指定数据的任何子集来检验是否重复。加入我们有一个额外的列，并想基于`k1`列去除重复值

```python
data['v1'] = range(7)
data.drop_duplicates(['k1'])
Out[8]: 
    k1  k2  v1
0  one   1   0
1  two   1   1
```

> 规则详见如下
>
> ```python
> >>> df
>     brand style  rating
> 0  Yum Yum   cup     4.0
> 1  Yum Yum   cup     4.0
> 2  Indomie   cup     3.5
> 3  Indomie  pack    15.0
> 4  Indomie  pack     5.0
> By default, it removes duplicate rows based on all columns.
> >>> df.drop_duplicates()
>     brand style  rating
> 0  Yum Yum   cup     4.0
> 2  Indomie   cup     3.5
> 3  Indomie  pack    15.0
> 4  Indomie  pack     5.0
> To remove duplicates on specific column(s), use subset.
> >>> df.drop_duplicates(subset=['brand'])
>     brand style  rating
> 0  Yum Yum   cup     4.0
> 2  Indomie   cup     3.5
> ```

`duplicated`和`drop_diplicated`默认保留第一个观测到的数值，传入参数`keep='last'`可以返回最有一个

```python
data.drop_duplicates(['k1','k2'],keep='last')
Out[9]: 
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
6  two   4   6
```

## 2. 使用函数或映射进行数据转换

在许多数据集中，你可能希望基于DataFrame中的数组，列或者列中的数值进行一些转换

```python
data = pd.DataFrame({
    'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'],
    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]
})
data
Out[3]: 
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     Pastrami     6.0
4  corned beef     7.5
5        Bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0

```

假设逆向添加一列用于表明每种事物的动物肉类型，让我们写下映射

```python
meat_to_animal = {
    'bacon': 'pig',
    'pulled pork': 'pig',
    'pastrami': 'cow',
    'corned beef': 'cow',
    'honey ham': 'pig',
    'nova lox': 'salmon'
}
```

Series的map方法接收一个函数或一个包含映射关系的字典型类型，这里有些小问题，就是肉类大写了，但一部分没有，这时候我们就需要使用`str.lower`方法进行转换

```python
lowercased = data['food'].str.lower()
lowercased
Out[6]: 
0          bacon
1    pulled pork
2          bacon
3       pastrami
4    corned beef
5          bacon
6       pastrami
7      honey ham
8       nova lox
Name: food, dtype: object
data['animal'] = lowercased.map(meat_to_animal)
data
Out[7]: 
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon

```

我们也可以传入一个能够完成所有工作的函数

```python
data['food'].map(lambda x:meat_to_animal[x.lower()])
Out[8]: 
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food, dtype: object
```

使用map是一种可以便捷执行按元素转换及其他清洗相关操作的方法

## 3. 替代值

使用fillna填充缺失值是通用值替换的特殊案例，你可以看到map可以用来修改一个对象的子集的值，但是replace更加灵活简单。

```python
data = pd.Series([1., -999, 2, -999., -1000, 3.])
data
Out[9]: 
0       1.0
1    -999.0
2       2.0
3    -999.0
4   -1000.0
5       3.0
dtype: float64
```

-999是一种缺失值的标识，如果想用NA来代替我们可以使用replace函数来生成一个Series(当然也可以使用replace=True)

```python
data.replace(-999,nan)
Out[10]: 
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64

```

如果我们想要一次传入多个值，。我们可以传入一个列表和一个替代之

```python
data.replace([-999,-1000],nan)
Out[11]: 
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64
```

要将不同的值替换为不同的值，可以传入替代之的列表

```python
data.replace([-999, -1000], [nan, 0])
Out[12]: 
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64
```

参数也可以使用字典来作为参数

```python
data.replace({-999: nan, 1000: 0})
Out[13]: 
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64

```

> data.replace 和data.str.replace是不同的，后者是对字符串进行逐元素的替代

## 4. 重命名轴索引

和Series中的一样，可以通过函数或某种形式的映射对轴标签进行类似的转换，生成新的但是带有不同标签的对象。但是也可以在不生产新的数据结构的情况下修改轴

```python
data = pd.DataFrame(np.arange(12).reshape(3, 4), index=['Ohio', 'Colorado', 'New York'],
                    columns=['one', 'two', 'three', 'four'])
transform = lambda x: x[:4].upper()
data.index.map(transform)
Out[15]: Index(['OHIO', 'COLO', 'NEW '], dtype='object')
```

你可以赋值给index来修改DataFrame

```python
data.index = data.index.map(transform)
data
Out[18]: 
      one  two  three  four
OHIO    0    1      2     3
COLO    4    5      6     7
NEW     8    9     10    11

```

如果你想要创建数据集转换后的版本，并且不修改原有的数据集，一个有用的方法是rename

```python
data.rename(index=str.title, columns=str.upper)
Out[19]: 
      ONE  TWO  THREE  FOUR
Ohio    0    1      2     3
Colo    4    5      6     7
New     8    9     10    11
```

值得注意的是rename可以结合字典型对象使用，为轴标签的子集提供新的值

```python
data.rename(index={'OHIO': 'INDIANA'},columns={'three':'peekaboo'})
Out[20]: 
         one  two  peekaboo  four
INDIANA    0    1         2     3
COLO       4    5         6     7
NEW        8    9        10    11
```

传入`inplace=True`可以原地修改

## 5. 离散化和分箱

连续值经常需要离散化，或者分离成箱子进行分析，假设你有某项研究中一组人群的数据，你想将他们进行分组，放入离散的年龄框中

```python
ages = [20, 22, 25, 27, 23, 21, 37, 31, 31, 61, 45, 41, 32]
```

我们需要将年龄分为18-25，26-35，36-60以及61以上等若干组，为了实现这个功能，我们使用pandas的cut

```python
ages = [20, 22, 25, 27, 23, 21, 37, 31, 31, 61, 45, 41, 32]
bins = [18, 25, 35, 60, 100]
cats = pd.cut(ages,bins)
cats
Out[4]: 
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 13
Categories (4, interval[int64, right]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
```

cut返回的对象是一个特殊的Categories对象，你看到的输出描述了pandas.cut计算出来的箱子，你可以里较为标识箱名的字符串数组，它内部包含一个categories数组，它指定了不同的类别名称以及codes属性中的ages数据标签

```python
cats.codes
Out[5]: array([0, 0, 0, 1, 0, 0, 2, 1, 1, 3, 2, 2, 1], dtype=int8)

cats.categories
Out[6]: IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], dtype='interval[int64, right]')
                               
pd.value_counts(cats)
Out[7]: 
(18, 25]     5
(25, 35]     4
(35, 60]     3
(60, 100]    1
dtype: int64                                            
```

注意，`value_counts`是对箱数量的计数。

与区间的数学符号一致，你可以传递`right=False`来改变那一边是封闭的

```python
pd.cut(ages, [18, 26, 36, 61, 100], right=False)
Out[8]: 
[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)]
Length: 13
Categories (4, interval[int64, left]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]

```

你可以通过向labels选项传递一个列表或数组来传入自定义的箱名

```python
group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
pd.cut(ages, bins, labels=group_names)
Out[9]: 
['Youth', 'Youth', 'Youth', 'YoungAdult', 'Youth', ..., 'YoungAdult', 'Senior', 'MiddleAged', 'MiddleAged', 'YoungAdult']
Length: 13
Categories (4, object): ['Youth' < 'YoungAdult' < 'MiddleAged' < 'Senior']
```

如果你传给cut整数个箱来替代显式的箱边，pandas将根据数据中的最小值和最大值计算出等长的箱，考虑一下均匀分局的数据被切成四份的情况

```python
data = np.random.randn(20)
pd.cut(data, 4, precision=2)
Out[10]: 
[(-1.25, -0.11], (-1.25, -0.11], (-0.11, 1.03], (-1.25, -0.11], (-1.25, -0.11], ..., (-0.11, 1.03], (-0.11, 1.03], (-0.11, 1.03], (-1.25, -0.11], (1.03, 2.17]]
Length: 20
Categories (4, interval[float64, right]): [(-1.25, -0.11] < (-0.11, 1.03] < (1.03, 2.17] <(2.17, 3.3]]
                                                                                           
```

` precision=2`这个选项将十进制精度限制在2位

qcut是一个与分箱密切相关的函数，它基于样本分位数进行分箱。取决于数据分布，使用cut通常不会使每个箱具有相同数据量的数据点。由于qcut使用样本的分位数，你可以通过qcut来获得等长的箱子

```python
data = np.random.randn(1000)
cats = pd.qcut(data, 4)
cats
Out[11]: 
[(-3.334, -0.665], (0.664, 2.71], (-0.0686, 0.664], (-3.334, -0.665], (0.664, 2.71], ..., (-0.0686, 0.664], (0.664, 2.71], (-3.334, -0.665], (0.664, 2.71], (-0.665, -0.0686]]
Length: 1000
Categories (4, interval[float64, right]): [(-3.334, -0.665] < (-0.665, -0.0686] < (-0.0686, 0.664] <
                                           (0.664, 2.71]]
                                            
                                            
                                            
pd.value_counts(cats)
Out[12]: 
(-3.334, -0.665]     250
(-0.665, -0.0686]    250
(-0.0686, 0.664]     250
(0.664, 2.71]        250
dtype: int64                                            
```

与cut类似，你可以传入自定义的分位数(0,1之间的数据，包括边)

```python
pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
Out[13]: 
[(-1.286, -0.0686], (1.257, 2.71], (-0.0686, 1.257], (-3.334, -1.286], (1.257, 2.71], ..., (-0.0686, 1.257], (-0.0686, 1.257], (-3.334, -1.286], (-0.0686, 1.257], (-1.286, -0.0686]]
Length: 1000
Categories (4, interval[float64, right]): [(-3.334, -1.286] < (-1.286, -0.0686] < (-0.0686, 1.257] <
                                           (1.257, 2.71]]
```

## 6. 检测和过滤异常值

这很大程度上使应用数组操作的事情。

```python
data = pd.DataFrame(np.random.randn(1000,4))
data
Out[4]: 
            0         1         2         3
0   -0.195270 -0.008817 -0.694507 -1.206097
1    0.303354 -0.658911 -1.555810 -0.831278
2    1.667430 -0.043884 -0.962706  0.606734
3    0.553163  0.490115 -0.049514  0.358041
4   -0.344960 -0.416418 -0.528825  2.050840
..        ...       ...       ...       ...
995  0.483199  1.287267  1.509760  1.365288
996  0.392375  0.825308  0.031335  1.411354
997  0.898740 -0.386491 -1.051767 -0.264796
998  0.810929 -2.019251  0.561367  1.954180
999 -1.399558 -0.643457  0.582511  0.586748
[1000 rows x 4 columns]
```

找出绝对值大于3的数

```python
data.describe()
Out[5]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean      0.049479    -0.008498    -0.012637     0.002424
std       1.009566     0.988624     1.000317     1.003925
min      -2.733700    -3.874777    -2.802795    -3.311953
25%      -0.634436    -0.670646    -0.685589    -0.675600
50%       0.058290    -0.012814    -0.022677    -0.001431
75%       0.724643     0.642078     0.678271     0.636468
max       3.449085     2.894341     3.359129     4.711016



col = data[2]
col[np.abs(col)>3]
Out[6]: 
209    3.359129
669    3.022049
773    3.271821
Name: 2, dtype: float64
```

要选出所有值大于3或小于-3的行，你可以对布尔值DataFrame使用any方法

```python
data[(np.abs(data)>3).any(1)]
Out[7]: 
            0         1         2         3
183  3.430554 -0.159716 -0.086102  1.016955
190 -1.819042 -3.874777  0.226964  1.434296
209  1.426856 -0.324214  3.359129 -0.613103
337  3.449085  0.854080  0.527839 -0.563779
398  3.423217 -0.534410  0.736627 -0.101192
537 -0.370416 -0.480734  0.122244  4.711016
567 -0.568163  1.316935 -0.401973 -3.148400
577 -0.195446 -3.275999  0.729648  3.250517
614 -0.270355  2.243108  1.324058  3.747555
669  0.183555 -0.097991  3.022049 -0.572877
721 -0.258651  0.029265 -1.041550 -3.311953
773  1.004215  0.392993  3.271821  0.123431

```

> DataFrame.any(*axis=0*, *bool_only=None*, *skipna=True*, *level=None*, ***kwargs*)
>
> Return whether any element is True, potentially over an axis.
>
> Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).
>
> 
>
> **axis**:{0 or ‘index’, 1 or ‘columns’, None}, default 0

选出了对应的值，那么我们就可以进行修改

```python
data[(np.abs(data) > 3)] = np.sign(data)*3


data.describe()
Out[10]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean      0.048176    -0.007348    -0.013290     0.000176
std       1.005464     0.984703     0.998245     0.992554
min      -2.733700    -3.000000    -2.802795    -3.000000
25%      -0.634436    -0.670646    -0.685589    -0.675600
50%       0.058290    -0.012814    -0.022677    -0.001431
75%       0.724643     0.642078     0.678271     0.636468
max       3.000000     2.894341     3.000000     3.000000
```

np.sign(data)会根据数据的正负生成1和-1

## 7. 置换和随机采样

使用`np.random.permutation`对DataFrame中的Series或行进行置换(随机重新排序)非常方便，在调用permutation时根据你想要的轴长度可以可以产生一个标识新顺序的整数数组

```python
df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))
sampler = np.random.permutation(5)
sampler
Out[12]: array([0, 4, 2, 1, 3])
```

整数数组可以用在基于iloc的索引或等价的take函数中

```python
df
Out[13]: 
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
df.take(sampler)
Out[14]: 
    0   1   2   3
0   0   1   2   3
4  16  17  18  19
2   8   9  10  11
1   4   5   6   7
3  12  13  14  15

```

要选出一个不含有替代之的随机子集，你可以使用Series和DataFrame的sample方法

```python
df.sample(n=3)
Out[15]: 
    0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
```

> n:Number of items from axis to return. Cannot be used with frac. Default = 1 if frac = None.

要生成一个带有替代值的随机子集(允许有重读选项)，将`replace=True`传入sample方法

```python
choices = pd.Series([5, 7, -1, 6, 4])
draws = choices.sample(n=10, replace=True)
draws
Out[16]: 
4    4
3    6
2   -1
1    7
2   -1
1    7
0    5
4    4
3    6
3    6
dtype: int64

```

## 8. 计算指标/虚拟变量

将分类变量转化为"虚幻"或"指标"矩阵是另一种用于通国际建模或机器学习的转化操作。

如果DataFrame中的一列由k个不同的值，则可以衍生一个k列的值为1和0的矩阵或DataFrame。

pandas与有一个get_dummies函数用于实现该功能。

```python
df = pd.DataFrame(
    {'key': ['b', 'b', 'a', 'c', 'a', 'b'],
     'data1': range(6)
     })

df
Out[20]: 
  key  data1
0   b      0
1   b      1
2   a      2
3   c      3
4   a      4
5   b      5


pd.get_dummies(df['key'])
Out[19]: 
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0

```

在某些情况下，你可能想在指标DataFrame的列上加上前缀，然后与其他数据合并，在get_dummies方法中有一个前缀参数用于实现该功能

```python
dummies = pd.get_dummies(df['key'], prefix='key')
df_with_dummy = df[['data1']].join(dummies)
df_with_dummy
Out[21]: 
   data1  key_a  key_b  key_c
0      0      0      1      0
1      1      0      1      0
2      2      1      0      0
3      3      0      0      1
4      4      1      0      0
5      5      0      1      0
```

如果DataFrame中的一行属于多个类别，则略微复杂

```python
mnames = ['movie_id', 'title', 'genres']
movies = pd.read_table('pydata-book-3rd-edition/datasets/movielens/movies.dat', sep='::', header=None, names=mnames)
movies[:10]

Out[22]: 
   movie_id                               title                        genres
0         1                    Toy Story (1995)   Animation|Children's|Comedy
1         2                      Jumanji (1995)  Adventure|Children's|Fantasy
2         3             Grumpier Old Men (1995)                Comedy|Romance
3         4            Waiting to Exhale (1995)                  Comedy|Drama
4         5  Father of the Bride Part II (1995)                        Comedy
```

为每个电影流派添加指标变量进行一些数据处理，首先，我们从数据集中提取出所有不同的流派的列表

```python
all_genres = []
for x in movies.genres:
    all_genres.extend(x.split('|'))
genres = pd.unique(all_genres)
genres
Out[23]: 
array(['Animation', "Children's", 'Comedy', 'Adventure', 'Fantasy',
       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',
       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',
       'Western'], dtype=object)
```

使用全0是构建DataFrame的一种方式

```python
zero_matrix = np.zeros((len(movies), len(genres)))
dummies = pd.DataFrame(zero_matrix, columns=genres)

```

现在遍历每一步电影，将dummies每一行的条目设置为1。为了实现该功能，我么们使用`dummies.columns`来计算每一个流派的列指标

```python
gen = movies.genres[0]
gen.split('|')
Out[26]: ['Animation', "Children's", 'Comedy']

dummies.columns.get_indexer(gen.split('|'))
Out[31]: array([0, 1, 2], dtype=int64)



dummies.columns.get_indexer(gen.split('|'))
for i, gen in enumerate(movies.genres):
    indices = dummies.columns.get_indexer(gen.split('|'))
    dummies.iloc[i, indices] = 1
# 只是后就可以和movies联合了
movies_windic = movies.join(dummies.add_prefix('Genr_'))
movies_windic.iloc[0]
Out[38]: 
movie_id                                      1
title                          Toy Story (1995)
genres              Animation|Children's|Comedy
Genr_Animation                                1
Genr_Children's                               1
Genr_Comedy                                   1
Genr_Adventure                                0
Genr_Fantasy                                  0
Genr_Romance                                  0
Genr_Drama                                    0
Genr_Action                                   0
Genr_Crime                                    0
Genr_Thriller                                 0
Genr_Horror                                   0
Genr_Sci-Fi                                   0
Genr_Documentary                              0
Genr_War                                      0
Genr_Musical                                  0
Genr_Mystery                                  0
Genr_Film-Noir                                0
Genr_Western                                  0
Name: 0, dtype: object

```

> 如果处理更大的数据，上面的速度就不是很快，更好的方法是直接将数据写为numpy数组的底层函数，然后封装进DataFrame

将get_dummires和cut等离散化函数结合使用时统计应用的一个有用方法

```python
np.random.seed(12345)
values = np.random.rand(10)
values
Out[39]: 
array([0.92961609, 0.31637555, 0.18391881, 0.20456028, 0.56772503,
       0.5955447 , 0.96451452, 0.6531771 , 0.74890664, 0.65356987])

bins = [0, 0.2, 0.4, 0.6, 0.8, 1]
pd.get_dummies(pd.cut(values,bins))
Out[40]: 
   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]
0           0           0           0           0           1
1           0           1           0           0           0
2           1           0           0           0           0
3           0           1           0           0           0
4           0           0           1           0           0
5           0           0           1           0           0
6           0           0           0           0           1
7           0           0           0           1           0
8           0           0           0           1           0
9           0           0           0           1           0


                                                    
vv = pd.DataFrame(values)
vv.join(pd.get_dummies(pd.cut(values,bins)))
Out[44]: 
          0  (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]
0  0.929616           0           0           0           0           1
1  0.316376           0           1           0           0           0
2  0.183919           1           0           0           0           0
3  0.204560           0           1           0           0           0
4  0.567725           0           0           1           0           0
5  0.595545           0           0           1           0           0
6  0.964515           0           0           0           0           1
7  0.653177           0           0           0           1           0
8  0.748907           0           0           0           1           0
9  0.653570           0           0           0           1           0
                                                    
```

