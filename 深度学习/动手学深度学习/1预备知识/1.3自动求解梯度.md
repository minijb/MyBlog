PyTorch提供的[autograd](https://pytorch.org/docs/stable/autograd.html)包能够根据输入和前向传播过程自动构建计算图，并执行反向传播

## 1. 概念

如果将tensor的`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。

> 注意在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`

如果不想要被继续追踪，可以调用`.detach()`将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（`requires_grad=True`）的梯度。

`Function`是另外一个很重要的类。`Tensor`和`Function`互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个`Tensor`都有一个`.grad_fn`属性，该属性即创建该`Tensor`的`Function`, 就是说该`Tensor`是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。

以下距离说明

## 2. Tensor

创建一个tensor并将`requires_grad=True`

```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
print(x.grad_fn)
'''
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
None
'''


y = x + 2
print(y)
print(y.grad_fn)

'''
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
<AddBackward0 object at 0x0000023667E27E80>
'''
```

直接创建的我们称为叶子节点，叶子节点对应的`grad_fn`是`None`。

```python
print(x.is_leaf, y.is_leaf) # True False
```

复杂运算

```python
z = y * y * 3
out = z.mean()
print(z)
print(out)
'''
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>)
tensor(27., grad_fn=<MeanBackward0>)
'''
```

通过`.requires_grad_()`来用in-place的方式改变`requires_grad`属性：

```python
a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False
a = ((a * 3) / (a - 1))
print(a.requires_grad) # False
a.requires_grad_(True)
print(a.requires_grad) # True
b = (a * a).sum()
print(b.grad_fn)#<SumBackward0 object at 0x0000023667E27E80>
```

## 3. 梯度

因为out是一个标量，所以调用`backward()`时不需要指定求导变量：

```python
x = torch.ones(2, 2, requires_grad=True)
z = y * y * 3
out = z.mean()
out#z.mean()
out.backward() # 等价于 out.backward(torch.tensor(1.))
print(x.grad)
'''
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
'''
```

$d(out) \over dx$,$o = {1 \over 4}\sum_{i=1}^4z_i={1 \over 4}\sum_{i=1}^43(x_i+2)^2$

如果有一个函数值和自变量都是像俩个的函数$\vec{y}=f(\vec{x})$那么结果就是一个**雅可比矩阵**
$$
J = \begin{pmatrix}
{\frac{\partial y_1}{\partial x_1}} &  ...& \frac{\partial y_1}{\partial x_n}\\ 
 ...&  & \\ 
 \frac{\partial y_m}{\partial x_1}&...  & \frac{\partial y_m}{\partial x_n}
\end{pmatrix}
$$
而`torch.autograd`这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 $v$ 是一个标量函数的 $l=g(y )l=g\left(\vec{y}\right)l=g(y) $的梯度：
$$
v= (\frac{\partial l}{\partial y_1}...\frac{\partial l}{\partial y_m})
$$
那么根据链式法则我们有$l$ 关于 $\vec{x}$ 的雅克比矩阵就为
$$
vl =(\frac{\partial l}{\partial y_1}...\frac{\partial l}{\partial y_m})\begin{pmatrix}
{\frac{\partial y_1}{\partial x_1}} &  ...& \frac{\partial y_1}{\partial x_n}\\ 
 ...&  & \\ 
 \frac{\partial y_m}{\partial x_1}&...  & \frac{\partial y_m}{\partial x_n}
\end{pmatrix} = (\frac{\partial l}{\partial x_1}...\frac{\partial l}{\partial x_n})
$$

> 注每次反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，因此每次求解梯度的时候都需要置0

```python
# 再来反向传播一次，注意grad是累加的
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
x.grad.data.zero_()
out3.backward()
print(x.grad)
```

### 3.1 为什么使用backward的参数

backward参数的作用其实就是简单的加权求和，根本原因**不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量**

```python
x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
y = 2 * x
z = y.view(2, 2)
print(z)
#现在 z 不是一个标量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量
v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)
z.backward(v)
print(x.grad)
```

> x.grad时和x同形状的张量

### 3.2 中断梯度追踪

```python
x = torch.tensor(1.0, requires_grad=True)
y1 = x ** 2
with torch.no_grad():
    y2 = x ** 3
y3 = y1 + y2

print(x.requires_grad)
print(y1, y1.requires_grad) # True
print(y2, y2.requires_grad) # False
print(y3, y3.requires_grad) # True
'''
True
tensor(1., grad_fn=<PowBackward0>) True
tensor(1.) False
tensor(2., grad_fn=<ThAddBackward>) True

'''

y3.backward()
print(x.grad)
'''
tensor(2.)
'''
```

注意，因为y2时不需要梯度下降，所以我们得到的梯度是2不是5

如果使用`y2.backward()`会报错

此外，如果我们想要修改`tensor`的数值，但是又不希望被`autograd`记录（即不会影响反向传播），那么我么可以对`tensor.data`进行操作。

```python
x = torch.ones(1,requires_grad=True)

print(x.data) # 还是一个tensor
print(x.data.requires_grad) # 但是已经是独立于计算图之外

y = 2 * x
x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播

y.backward()
print(x) # 更改data的值也会影响tensor的值
print(x.grad)

'''
tensor([1.])
False
tensor([100.], requires_grad=True)
tensor([2.])

'''
```

