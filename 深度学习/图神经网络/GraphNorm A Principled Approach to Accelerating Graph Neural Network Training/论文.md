**问题**

- the optimization of GNNs is less well understood,
- the training of GNNs is often unstable and the convergence is slow 

**本文目的**

- we study how to improve the training of GNNs via normalization

**使用现有的正则方法进行实验**

- InstanceNorm 的效果最好

**本文主要内容**

- 常见的batchNorm没有带来很好效果的原因是，如果在batch-level上进行统计的话和对单一图进行标准化，方差会大很多，噪音回到是优化不稳定

- InstanceNorm 中在端点隐藏的表达中减去平均值可能导致GNN表达退化，一方面原因就是平均值统计信息记录着全图的结构信息，如果移除他们可能对破坏表达能力
- The learnable shift could learn to control the ideal amount of information to preserve for mean statistics.

## 1. 预处理

### 1.1 符号介绍

![image.png](https://s2.loli.net/2022/10/07/vtXTIcGJbPAdWFY.png)







