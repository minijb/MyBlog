##  1. 现在的问题

- 我们当前的输入只是一个vector
- 如果我们输入的是a set of vectors,and may change length

最典型的例子就是文字输入，每个词语的特征向量可能不一样，graph的节点特征，原子的特征等等等

此时output是什么样的呢？

<img src="https://s2.loli.net/2022/10/23/nhvrYgB8HPtoU9f.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/23/b7AVifhW6de2RYL.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/23/VFn6qftAo43Ogkc.png" alt="image.png" style="zoom:33%;" />

此时我们以文字识别为例，此时我们有一个句子，我们可以选择使用全连接来进行训练，如下图，saw作为名词和动词来说应该是不一样的，但是全连接层会把他判定成一样的，因为没有考虑上下文信息，我们可以选择设置一个windows，但是这样的话我们就会有很大的参数量

<img src="https://s2.loli.net/2022/10/23/IgNpSaTE7GRnuZO.png" alt="image.png" style="zoom:33%;" />

此时我们就可以引入自注意力机制

<img src="https://s2.loli.net/2022/10/23/Hnz6wBFNbIc1PXq.png" alt="image.png" style="zoom:33%;" />

self-attention是考虑一整个sequence之后得出的结果！！我们叠加多个self-attention机制

<img src="https://s2.loli.net/2022/10/23/VgIkts4Jfce95Xu.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/23/ACM561Gcln3Zw9g.png" alt="image.png" style="zoom:33%;" />

首先我们会计算出各个节点之间的关系分数，通过softmax得到

q:quert 	k:key 	 `q*k=a`“：分数

<img src="https://s2.loli.net/2022/10/23/XsjhJug2d4B6cpK.png" alt="image.png" style="zoom:33%;" />

然后我们会根据之前attention的分数抽取重要的信息

<img src="https://s2.loli.net/2022/10/24/MOgrT4L9WvoDC7S.png" alt="image.png" style="zoom: 50%;" />

<img src="https://s2.loli.net/2022/10/24/oSlUHizJugQVCXc.png" alt="image.png" style="zoom:33%;" />

我们从矩阵的角度看一下这个问题

<img src="https://s2.loli.net/2022/10/24/NAlxPMeDYrGCpOt.png" alt="image.png" style="zoom:33%;" />

我们可以直接把这个扩展为一个矩阵，如下图所示

<img src="https://s2.loli.net/2022/10/24/eLxnHRDN46XrdP2.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/tmA1IvqEBrz9MRh.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/WhnzvHCsGXSlq5p.png" alt="image.png" style="zoom:33%;" />

**一个比较经典的进阶版本--Multi-head Self-attention**

一句话的格式可能不同，那么我们给出的q应该也有很多种

<img src="https://s2.loli.net/2022/10/24/OqKADQePG96mhua.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/pLEsK3Mot2Ddu9R.png" alt="image.png" style="zoom:33%;" />

现在有个问题就是我们缺少一些位置信息：我们只是人工标注了一下信息，但实际上这些信息之间的位置信息，因为只是进行矩阵的乘法，不能凸显位置信息，此时我们就可以用到positional encoding 技术

最开始这个位置向量是认为设置的

<img src="https://s2.loli.net/2022/10/24/pLEsK3Mot2Ddu9R.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/cqvFy8XzYgiksZ6.png" alt="image.png" style="zoom:33%;" />

有的时候，一阵整句话的参数连可能很多，此时我们就可以使用`Truncated self-attion`，这样只考虑用户自己设置的长度，而不是一整句话的长度

<img src="https://s2.loli.net/2022/10/24/lZ9DzrWdoemYutV.png" alt="image.png" style="zoom:33%;" />

同理再图像领域，我们也可以使用transformer，输入一整排向量

<img src="https://s2.loli.net/2022/10/24/hkJ9GaMZPqCsBV6.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/vEoY7WIxbAHCwOV.png" alt="image.png" style="zoom:33%;" />

## 2. self-attention和cnn

CNN考虑一个卷积核内的信息，而self-attention考虑全局的信息

<img src="https://s2.loli.net/2022/10/24/4hGQrjzokCPeWAq.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/FYaCue7t2j3lgpw.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/hZFiKQbOpAP8YJd.png" alt="image.png" style="zoom:33%;" />

## 3. RNN和self-attention

<img src="https://s2.loli.net/2022/10/24/ybR6SjOPoWcJvFk.png" alt="image.png" style="zoom:33%;" />

## 4. self-attention和graph

<img src="https://s2.loli.net/2022/10/24/JY8OepW2ocdN6QE.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/gAatDPMndsuqQxV.png" alt="image.png" style="zoom:33%;" />

<img src="https://s2.loli.net/2022/10/24/oaOGbQsyu76hIC1.png" alt="image.png" style="zoom:33%;" />